{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a17eb784-0612-475e-b868-0eb702da6f4b",
   "metadata": {},
   "source": [
    "# RAG example with Langchain, Milvus, and vLLM\n",
    "\n",
    "This Jupyter notebook is setting up a Retrieval-Augmented Generation (RAG) chatbot using LangChain, Milvus (a vector database), and an inference server hosting Mistral-7B-Instruct-v0.2. \n",
    "\n",
    "### Requirements:\n",
    "\n",
    "A Milvus instance, either standalone or cluster.\n",
    "Connection credentials to Milvus must be available as environment variables: MILVUS_USERNAME and MILVUS_PASSWORD.\n",
    "A vLLM inference endpoint. In this example we use the OpenAI Compatible API.\n",
    "Needed packages and imports\n",
    "\n",
    "## Step 01 Install required libraries\n",
    "Installs necessary libraries:\n",
    " - langchain: For managing LLM workflows.\n",
    " - pymilvus: For connecting to Milvus (a vector database).\n",
    " - sentence-transformers: For text embeddings.\n",
    " - openai: For interacting with OpenAI-compatible inference servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8359fa77-24ca-40f8-ae2b-b1dbac0728f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q einops==0.7.0 langchain==0.1.9 pymilvus==2.3.6 sentence-transformers==2.4.0 openai==1.13.3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f5662-be97-49d8-bb06-a6b3873e0f0a",
   "metadata": {},
   "source": [
    "## Step 02 Test connection to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b6cb8b-6975-4b84-a15c-5a1cb8938037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   497  100   497    0     0  33133      0 --:--:-- --:--:-- --:--:-- 33133\n",
      "{\n",
      "    \"object\": \"list\",\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"id\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "            \"object\": \"model\",\n",
      "            \"created\": 1752067307,\n",
      "            \"owned_by\": \"vllm\",\n",
      "            \"root\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "            \"parent\": null,\n",
      "            \"permission\": [\n",
      "                {\n",
      "                    \"id\": \"modelperm-3ef50ce2bf294e4698f3b95037d2e256\",\n",
      "                    \"object\": \"model_permission\",\n",
      "                    \"created\": 1752067307,\n",
      "                    \"allow_create_engine\": false,\n",
      "                    \"allow_sampling\": true,\n",
      "                    \"allow_logprobs\": true,\n",
      "                    \"allow_search_indices\": false,\n",
      "                    \"allow_view\": true,\n",
      "                    \"allow_fine_tuning\": false,\n",
      "                    \"organization\": \"*\",\n",
      "                    \"group\": null,\n",
      "                    \"is_blocking\": false\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl  https://vllm-vllm.apps.lisbon-sgioia-01.lis.ciscodemo.int/v1/models | python -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530c7b0-589a-433b-b5f3-48ab633f70f5",
   "metadata": {},
   "source": [
    "## Step 3 Import Libraries & Set Up Configuration\n",
    "\n",
    "Imports necessary components from langchain, including:\n",
    " - RetrievalQA: Enables retrieval-augmented question-answering.\n",
    " - Milvus: Connects to Milvus for vector search.\n",
    " - VLLMOpenAI: Uses an inference server compatible with OpenAI’s API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c087eae-4176-4d91-b735-c3ee39d8c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb0cb4-0c35-46c0-aedd-d397b3571a7d",
   "metadata": {},
   "source": [
    "## Step 4 Bases parameters, Inference server and Milvus info\n",
    "\n",
    "Define Model & Milvus Vector Database Configuration.\n",
    " - LLM Configuration:\n",
    "   - Uses an OpenAI-compatible inference server (VLLM).\n",
    "   - Loads the Mistral-7B-Instruct-v0.2 model.\n",
    "   - Sets hyperparameters like temperature (controls randomness) and top_p (nucleus sampling).\n",
    " - Milvus Vector Database Configuration:\n",
    "   - Connects to a Milvus instance (vectordb-milvus).\n",
    "   - Uses a collection named \"splunk_appdynamics\" for storing and retrieving embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0965552b-e73e-4a27-985d-5d97c85c0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SERVER_URL = \"https://vllm-vllm.apps.lisbon-sgioia-01.lis.ciscodemo.int/v1\"\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "MAX_TOKENS=100\n",
    "TOP_P=0.95\n",
    "TEMPERATURE=0.5\n",
    "PRESENCE_PENALTY=1.03\n",
    "MILVUS_HOST = \"vectordb-milvus.milvus.svc.cluster.local\"\n",
    "MILVUS_PORT = 19530\n",
    "MILVUS_USERNAME = os.getenv('MILVUS_USERNAME')\n",
    "MILVUS_PASSWORD = os.getenv('MILVUS_PASSWORD')\n",
    "MILVUS_COLLECTION = \"Football\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd88d8b-9af2-49ce-89f2-9602a45e6f61",
   "metadata": {},
   "source": [
    "## Step 5 Load Embeddings Model\n",
    "\n",
    "Uses nomic-ai/nomic-embed-text-v1 from Hugging Face for text embeddings. These embeddings are used to convert text into numerical vectors for similarity search in Milvus.\n",
    "Then connect to Milvus as a vector database to:\n",
    " - Stores documents as embeddings.\n",
    " - Allows fast similarity search for retrieval-augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a45f093d-a8af-4f66-bd0d-afb91919db47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.4.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {'trust_remote_code': True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT, \"user\": MILVUS_USERNAME, \"password\": MILVUS_PASSWORD},\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    metadata_field=\"metadata\",\n",
    "    text_field=\"page_content\",\n",
    "    drop_old=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534bbb14-9835-4b84-b74c-6ec47334e88d",
   "metadata": {},
   "source": [
    "## Step 6 Define the Prompt Template & create a Retrieval-QA Chain \n",
    "\n",
    "Define the prompt template:\n",
    "Then uses VLLMOpenAI to connect to an inference server running Mistral-7B.\n",
    " - Enables streaming responses.\n",
    " - Uses the API base URL (INFERENCE_SERVER_URL).\n",
    "\n",
    "In this section, the notebook combines a retrieval system with a language model (LLM) to create a question-answering (QA) pipeline. This is done using LangChain’s RetrievalQA class, which integrates document retrieval with LLM-based generation. The goal is to improve the LLM’s responses by retrieving relevant documents from a vector database (Milvus) before generating an answer.\n",
    "Instead of relying solely on the model’s pre-trained knowledge, RAG ensures factual correctness by referencing real-time data.\n",
    "The RetrievalQA chain first retrieves the most similar documents from Milvus and then passes them as context to Mistral-7B for response generation.\n",
    "\n",
    " Create a Retrieval-QA Chain. Combines:\n",
    " - Milvus (retriever) for document search.\n",
    " - Mistral-7B (LLM) for answer generation.\n",
    " - Uses similarity search (k=4) to retrieve the most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c5b46f8-4881-4930-805d-89c958c70ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST] <<SYS>>\n",
    "You are CiscoBot, a helpful, respectful, and knowledgeable assistant.\n",
    "\n",
    "Your task is to answer questions based on the context provided. When responding:\n",
    "\n",
    "Focus on clarity and helpfulness: Provide answers that are informative, accurate, and as detailed as necessary to address the question effectively.\n",
    "Be respectful and positive: Ensure your responses are polite, considerate, and promote a positive experience for the user.\n",
    "Safety is key: Avoid sharing harmful, unethical, discriminatory, or illegal content in your answers. Always prioritize social responsibility and inclusivity in your responses.\n",
    "Explain when unsure: If a question is unclear or not factually sound, offer a polite explanation of why and avoid providing incorrect or speculative information.\n",
    "Always aim to provide the most reliable and accurate information, ensuring your responses are constructive and relevant to the user's inquiry.\n",
    "<</SYS>>\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "import httpx\n",
    "\n",
    "llm =  VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=INFERENCE_SERVER_URL,\n",
    "    model_name=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    presence_penalty=PRESENCE_PENALTY,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    async_client=httpx.AsyncClient(verify=False),\n",
    "    http_client=httpx.Client(verify=False)\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "            ),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True\n",
    "        )\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4983a4-56d9-4c4e-abf8-758825f7be7e",
   "metadata": {},
   "source": [
    "## Step 7 Query Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a9ef22f-4919-4c1c-b550-71a0c7d493a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The PFA (Professional Footballers' Association) Player of the Year Award in 2024 was won by Phil Foden. However, the context provided does not mention the PFA Premier League Team of the Year. To answer that question, I would need to access more information or clarify which league's team you are asking about."
     ]
    }
   ],
   "source": [
    "question = \"Who was the winner of the PFA Premier League Team of the Year in 2024?\"\n",
    "result = qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea378e9-7016-439a-aade-7a375dcba427",
   "metadata": {},
   "source": [
    "## Step 8 Get Sources\n",
    "\n",
    "Removes duplicate documents based on their metadata source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f95d5e4d-77ff-42b1-b902-e45e2d29c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.transfermarkt.com/\n",
      "https://www.fabrizioromano.org/\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item.metadata['source'] not in unique_list:\n",
    "            unique_list.append(item.metadata['source'])\n",
    "    return unique_list\n",
    "\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "\n",
    "for s in results:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5485b-e51b-4a88-b45f-af78da7fe63e",
   "metadata": {},
   "source": [
    "## Step 9 Install Gradio\n",
    "\n",
    "Gradio is an open-source Python library for creating user-friendly web interfaces for machine learning models and applications. It allows developers to quickly build interactive demos with just a few lines of code. Users can input text, images, or other data types and receive real-time responses from models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f118440-7f3e-44e2-9035-8b61ba182c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917fa1e-6a74-46c3-ae1d-51e687e5d83b",
   "metadata": {},
   "source": [
    "## Step 10 Launch Gradio Chatbot\n",
    "Creates a Gradio UI:\n",
    " - Allows users to enter questions.\n",
    " - Uses retrieval-augmented generation (RAG) to provide answers.\n",
    " - Launches the chatbot (local-only, not public)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499d37a-a133-4d99-8de0-adab805a7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def rag_query(user_input, _=None):  # Add `_` to ignore the extra argument\n",
    "    response = qa_chain.invoke(user_input)\n",
    "    result = response['result']\n",
    "    return result\n",
    "\n",
    "import json\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=rag_query,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    "    title=\"🔍 RAG Chatbot\",\n",
    "    description=\"Ask a question, and the chatbot will retrieve relevant documents before answering.\"\n",
    ")\n",
    "\n",
    "\n",
    "demo.launch(share=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f12121-abc3-4682-8c7c-c73ad557de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "\n",
    "# INFERENCE_SERVER_URL = \"https://vllm-vllm.apps.lisbon-sgioia-01.lis.ciscodemo.int/v1\"\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "def rag_query(user_input, _=None):  # Add `_` to ignore the extra argument\n",
    "    response = qa_chain.invoke(user_input)\n",
    "    return response\n",
    "\n",
    "# Function to query your LLM using vLLM API\n",
    "def query_llm(prompt):\n",
    "    api_url = \"https://vllm-vllm.apps.lisbon-sgioia-01.lis.ciscodemo.int/v1/chat/completions\"\n",
    "\n",
    "    # Correcting the payload structure\n",
    "    data = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(response)\n",
    "        response_json = response.json()\n",
    "        return response\n",
    "\n",
    "# Create a Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=query_llm,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"LLM Query Interface\",\n",
    "    description=\"Enter your prompt to query the language model.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6f706-0fad-434b-9139-2069fade1f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
